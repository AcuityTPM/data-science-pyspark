{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col, array_contains\n",
    "\n",
    "# Get or instantiate a SparkContext and register it as a singleton object\n",
    "sc = SparkSession.builder.appName('adq').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- Date: string (nullable = true)\n |-- Open: double (nullable = true)\n |-- High: double (nullable = true)\n |-- Low: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Adj Close: double (nullable = true)\n |-- Volume: integer (nullable = true)\n\n+----------+---------+---------+---------+---------+---------+--------+\n|      Date|     Open|     High|      Low|    Close|Adj Close|  Volume|\n+----------+---------+---------+---------+---------+---------+--------+\n|2010-06-29|     19.0|     25.0|17.540001|23.889999|23.889999|18766300|\n|2010-06-30|25.790001|    30.42|23.299999|    23.83|    23.83|17187100|\n|2010-07-01|     25.0|    25.92|    20.27|21.959999|21.959999| 8218800|\n|2010-07-02|     23.0|     23.1|18.709999|19.200001|19.200001| 5139800|\n|2010-07-06|     20.0|     20.0|    15.83|16.110001|16.110001| 6866900|\n|2010-07-07|     16.4|16.629999|    14.98|     15.8|     15.8| 6921700|\n|2010-07-08|16.139999|    17.52|    15.57|17.459999|17.459999| 7711400|\n|2010-07-09|    17.58|     17.9|16.549999|     17.4|     17.4| 4050600|\n|2010-07-12|17.950001|    18.07|     17.0|17.049999|17.049999| 2202500|\n|2010-07-13|17.389999|18.639999|     16.9|18.139999|18.139999| 2680100|\n|2010-07-14|17.940001|    20.15|    17.76|    19.84|    19.84| 4195200|\n|2010-07-15|19.940001|     21.5|     19.0|19.889999|19.889999| 3739800|\n|2010-07-16|20.700001|21.299999|20.049999|20.639999|20.639999| 2621300|\n|2010-07-19|21.370001|    22.25|    20.92|    21.91|    21.91| 2486500|\n|2010-07-20|    21.85|    21.85|20.049999|20.299999|20.299999| 1825300|\n|2010-07-21|    20.66|     20.9|     19.5|20.219999|20.219999| 1252500|\n|2010-07-22|     20.5|    21.25|20.370001|     21.0|     21.0|  957800|\n|2010-07-23|21.190001|21.559999|21.059999|21.290001|21.290001|  653600|\n|2010-07-26|     21.5|     21.5|20.299999|20.950001|20.950001|  922200|\n|2010-07-27|    20.91|    21.18|    20.26|20.549999|20.549999|  619700|\n+----------+---------+---------+---------+---------+---------+--------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "delimiter = \",\"\n",
    "# A DataFrame is created by reading a file\n",
    "df = sc.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", delimiter).load(\"TSLA.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science PySpark Notebook\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------+---------+-----+---------+---------+---------+--------+\n|Date      |Open     |High |Low      |Close    |Adj Close|Volume  |\n+----------+---------+-----+---------+---------+---------+--------+\n|2010-06-29|19.0     |25.0 |17.540001|23.889999|23.889999|18766300|\n|2010-06-30|25.790001|30.42|23.299999|23.83    |23.83    |17187100|\n|2010-07-01|25.0     |25.92|20.27    |21.959999|21.959999|8218800 |\n|2010-07-02|23.0     |23.1 |18.709999|19.200001|19.200001|5139800 |\n|2010-07-06|20.0     |20.0 |15.83    |16.110001|16.110001|6866900 |\n+----------+---------+-----+---------+---------+---------+--------+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "\"\"\" \n",
    "There is some anxiety present, the fear of failing, the fear of being overwhelmed, the fear of not understanding\n",
    "Please let's try to work through it ; knee is shaking, h feelings are arising\n",
    "Very cool, the morning anxiety inducing meeting has passed, and I'm ready to work. From this point out, this is my full time job, and I plan to thrive. So let's put in 12 hours a day moving forward and get ready to impress. You have about 3 weeks of work to make up. Which is 120 hours, 4 hours a day and 16 hours on the weekends. We need to move. Fast\n",
    "\n",
    "why won't some output show up unless it's in a new cell?\n",
    "\n",
    "Column instance: df.colName; almost all functions from the pyspark.sql.functions module take one or more column instances as arguments. These functions are important for data manipulation\n",
    "\"\"\"\n",
    "\n",
    "df.show(n=5, truncate=False)\n",
    "\n",
    "# pandas DataFrame -> spark DataFrame\n",
    "# pdf = pd.read_csv('TSLA')\n",
    "# df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Date', 'string'),\n ('Open', 'double'),\n ('High', 'double'),\n ('Low', 'double'),\n ('Close', 'double'),\n ('Adj Close', 'double'),\n ('Volume', 'int')]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593577208767",
   "display_name": "Python 3.7.7 64-bit ('dsenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}